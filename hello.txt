Enhancing Chinese Word Embeddings from Latent Meanings of Main-Components in Chinese Characters

Abstract  
%We propose MCWE,a novel method for learning Chinese word embeddings.
Word embeddings have a great impact on natural language processing.
In morpheme writing systems,most Chinese word embeddings takes a word as the basic unit, or directly use the internal structure of words.
However,these models still neglect the rich latent meanings in the internal structure of Chinese characters.
According to our observations, the latent meanings of the main-components in Chinese characters are very helpful for improving Chinese word embeddings learning.
In this paper,we explore to employ the latent meanings of the main-components of the Chinese characters to train and enhance the Chinese word embeddings.
Based on this purpose,we propose two main-component enhanced word embedding models(MCWEs),named MCWE-SA and MCWE-HA respectively,
which incorporate the latent meanings of the main-components during the training process.
Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our models.
Evaluations on both word similarity and word analogy tasks demonstrate the superior performance of our models.

1.Introduction
  Word embedding, which is also termed distributed word representation, has been an important foundation in the field of Natural Language Processing.
It encodes the semantic meaning of a word into a real-valued low-dimensional vector,which performs better in many tasks such as text classification,
machine translation ..... (and so on) over traditional one-hot representations.
Among many word embedding models,Continuous Bag-of-Word (CBOW), Skip-gram (Mikolov et al.,2013a), Global Vectors (GloVe) (Penningtonet al., 2014) is popular 
because of their simplicity and efficiency.
The idea of those algorithms are mainly based on the distributed hypothesis which means words that are used and occur in the same contexts tend to purport similar meanings. 
[Harris, Z. (1954). "Distributional structure". Word. 10 (23): 146–162. doi:10.1080/00437956.1954.11659520].
However, these models only focus on word level information,and do not pay attention to the fine-grained morphological information inside the words or the characters,
such as components of Chinese characters or English morphemes.

  Different from English words,Chinese characters are glyphs whose components may depict objects or represent abstract notions.
Usually a character consists of more commonly two or more components which may have meanings related to the character,
using a variety of different principles[wiki written Chinese https://en.wikipedia.org/wiki/Written_Chinese]
That means Chinese words themselves are often composed of Chinese characters and subcharacter compoents,including rich semantic information.
    (引用论文)（过去式）
 
  Previous researchers have done some works by using the rich information inside Chinese for word embeddings enhancement with internal morphological semantics.
(Li et al.  Component-Enhanced Chinese Character Embeddings) used the radicals to enhance the Chinese character embeddings.
(Chen et al. Joint Learning of Character and Word Embeddings) proposed the CWE model to improve the quality of Chinese word embeddings by 
exploiting character level information.   
For a more fine-grained combination of Chinese character and radical,
(Yin et al.Multi-Granularity Chinese Word Embedding)proposed methods to enhance Chinese character embeddings based on CWE model.
(Jian et al.Improve Chinese Word Embeddings by Exploiting Internal Structure) used external language to calculate the similarity between Chinese words and characters 
to enhance semantic information based on the rich internal structure of Chinese words.
(huang er al. Learning Chinese Word Representations From Glyphs Of Characters)proposed the GWE model,a pixel-based model that learns character features from font images to enhance the representation (of words with character glyphs).
(Yu et al.Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components) used Chinese characters and subcharacter components to improve Chinese word embeddings 
and proposed the JWE model to jointly learn Chinese word and character embeddings.
(Cao et al. cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information)proposed a cw2vec model, which exploits stroke-level information to improve the learning of Chinese word embeddings.

   However, the subcharacter compoents of a character contain a lot of unwanted noise information,
so we explored a new direction to integrate several word components into main-component.
%Chinese characters contain elements and radicals，just as Chinese contains Chinese characters.
Chinese characters are composed of components and radicals,and a component of the complex subword item can be a simple Chinese character.
In our model,the subcharacter components of characters can be roughly divided into two types: main-component and radical.
The main-component which consists of several components indicates the baisc meaning of a character while the radical indicates some attribute meanings of a character.
For example,"智 (intelligence)” is divided into the subcharacter components "矢(arrow)","口(mouth)" and "日(sun)" 
where "矢(arrow)" and "口(mouth)" are the components and 日(sun) is the radical of character "智 (intelligence)".
All these subcharacter components mentioned above may be not relevant to the semantics of the character "智 (intelligence)".
However, the main-component "知(wisdom)" consisting of several small components is closely related to the meaning of the character. 
%Based on this, we have proposed .....

    In this paper, we have modified the CBOW model in the word2vec source code[https://code.google.com/p/word2vec], 
introduced the information of the latent meaning of the Chinese main-component and radical, 
and proposed two efficient models called MCWE-SA and MCWE-HA.
The learned Chinese word embeddings not only encode morphological properties into Chinese words, but also have a higher similarity to the synonym.
Our model directly modifies embeddings of the target words, without generating and training extra embeddings for latent meanings.
In addition,we create a word map to describe the relationship between Chinese words and the latent meanings of their main-components and radicals.


Through our model, the rich implicit information in Chinese is fully utilized and the similarities between words and words is improved.%
%//Using our model, we can strengthen the similarity between words and words more effectively.
%（贡献）1.构建汉语主成分map表；2.attention机制引用 3.有很强的拓展性（CWE）
Our contributions of this paper can be summarized as follows:

1.Rather than directly leverage the components of the word itself, we provide a method to use the latent meanings of the main-components and radicals to train the word embedding.
In order to verify the feasibility of our purpose,two models,named MCEW-SA and MCWE-HA,are proposed to incorporate the latent meanings.

2.We propose a method to assign the weights of latent meanings at input layer based on the attention scheme.
Through the attention mechanism, we focus on the latent meanings of higher contribution to the target word and assign them higher weights.

3.expriment
We evaluate the quality of word embedding learned by our models and the state-of-the-art models, using a medium-sized corpus through word similarity tasks and word analog tasks.
The results show that all of our models have performance improvements and MCWE-HA outperforms all baselines on the word analog task.
*******************************

2.My model(map表需要精确的标注隐含词义,对模型影响很大)
In this section, we introduce our Main-Component Word Embedding models(MCWEs),named MCWE-Soft Attention(MCWE-SA) and MCWE-Hard Attention(MCWE-HA),
which is based on CBOW model(Mikolov et al., 2013a).
It should be noted that our models directly uses the internal meanings of the Chinese characters,rather than directly using the characters or components of the word themselves.
Particularly,we treat the radicals as the main-components and have the same contribution in our model.
%(Particularly,we treat the radical as a main-component and add the latent meaning of the radical to the main-component latent meaning.)
MCWE-Avg assumes that all the latent meanings of the Chinese character's main-component to have the same weight.
However, most of the main-components are also a frequently-used Chinese word which may contain some ambiguous information.
To address this concern,we proposed the MCWE-SA which based on the soft attention scheme.
The MCWE-SA assumes that the latent meanings of main-components have their own weights,and assign higher weights to the meanings closely related to the target character,
so that they have greater impact on the word embeddings.
What's more,MCWE-HA which based on the hard attention scheme only focuses on the latent meaning of the main-component with the highest similarity to the target word.
In what follows, we will introduce each of our MCWEs in detail.
At the end of this section, we are going to introduce the update rules of the models.


MCWE-SA：

%We denote the Chinese character set as C and the Chinese word vocabulary as W. Each character ci-C is represented by vector x_i, and each word wi-W is represented by vector wi.





Through observation, we found that most Chinese words have more than one latent meaning with their main-component,
but some latent meanings have low correlation with the target word(the corresponding word).For example,main-component "知(knowledge)" means "" 、"" and "".
As Fig.3 shows,for the item "智慧(intelligence)" ——{[],[],[],[],[],[],[],[]} in the word map,each latent meaning has a bias on the word "智慧(intelligence)".
Therefore, we assign different weights to each latent meaning based on the idea of soft attention model.
We measure the weights of latent meanings by calculating the cosine similarity between the corresponding latent meanings and the target word where cosine similarity is usually used to measure the similarity between word embeddings.

We denote D as the training corpus, W =(w_1;w_2;.....;w_n) as the vocabulary of words,M_i=(m_1;m_2;...m_k) as the latent meanings in the word map for each words.
%Each word w_i(i\in{[1,n]}) has its corresponding latent meanings in the word map.
The item for w_i in the word map is w_i\mapsto M_i where M_i is a collection of the latent meanings of w_i's main-components .
We denote sim(·) as a method to measure the similarity between Chinese words and their latent meanings.

%Let Sim(·) denotes the function to calculate the similarity between meanings of Chinese words and characters, we use cosine distance as the distance metric.
Furthermore,we remove the negatively correlated latent meaning.
Hence, at the input layer, the modified embedding of w_i can be expressed as
%Formally, a context word wj is represented as
%The i-th latent meaning of the main-component of Chinese character c are ci.


\begin{equation}
\begin{aligned}
\hat v_{x_{i}} &= \frac{1}{2}\{v_{x_{i}}+\frac{1}{N_i}\sum_{k=1}^{N_i}sim(x_i,m_k)\cdot m_k  \},\\
sim(&x_i,m_k)=cos(v_{x_i},v_{m_k}), \quad  s.t. \    cos(v_{x_i},v_{m_k}) > 0
%a\overset{~}{x}b
\end{aligned}
\end{equation}


where v_{w_i} is the original word embedding of w_i,\hat v_{x_{i}} is the the modified word embedding of w_i and 
%v_{m_k} is the implied word embedding that positively contributes to wi
v_{m_k} indicates the embedding of latent meaning m_k that positively contributes to w_i.
N_i denotes the number of v_{m_k} whose cosine similarity between v_{x_i} is greater than 0.





MCWE-HA:
//In order to further reduce the problem that the main component implies the opposite of the similarity of Chinese characters, we propose MCWE-HA.
In order to further reduce the impact of irrelevant latent meanings to the word, we propose MCWE-HA which is based on the idea of hard attention model.
We only choose the latent meaning which has the greatest cosine similarity to the Chinese word.
According to experimental experience, we only retain the latent meaning of similarity with token w_i greater than 0.9.
And MCWE-HA is mathematically defined as

\begin{equation}
\begin{aligned}
\hat v_{x_{i}} &= \frac{1}{2}\(v_{x_{i}}+v_{m_{max}} \),\\
m_{max}&=\mathop{argmax}\limits_{M} cos(v_{x_i},v_{t}),  t\in{M_i}  \\
 s.t. \qquad      cos(&v_{x_i},v_{m_k}) > 0.9
\end{aligned}
\end{equation}

where m_{max} denotes the embedding of latent meanings which has the greatest cosine similarity to w_i.
%where we only keep values with cos（Vxi,Vm） greater than 0.5 for comparison. 



*******************************
3.Experiments
In this section,we test the performance of our model in generating high-quality word embeddings on word similarity evaluation and word analogy tasks.

3.1 Experimental Settings

Traing Corpus : 
We adopt a medium-sized Chinese corpus which is downloaded from Chinese Wikipedia Dump\footnote[1]{\url{https://download.wikipedia.com/zhwiki}} to train all word embedding models.
%We adopt the Chinese Wikipedia Dump as our training corpus whose size is about 7GB.
We utilize a script named WikiExtractor to convert data from XML into text format.What's more,We use THULAC 3 (Sun et al.,2016b) for Chinese word segmentation and normalize all characters as simplified Chinese.
In pre-processing,we filter all digits,punctuation masks and non Chinese characters.
To get better quality of the word embeddings, we removed the stop words (that are common_ which from *********) in the corpus.
Finally, we obtained a training corpus of approximately 2g in size, containing 354707204 words, and 1090983 unique words.

Word Map:

The Modern Chinese Word List is divided into two parts: 2500 common characters and 1000 secondary common characters,
and the coverage of those words in most corpora reached 99.48% which means mastering the common and secondary words has reached the basic requirements for using Chinese\footnote[]{\url{https://en.wikipedia.org/wiki/List_of_Commonly_Used_Characters_in_Modern_Chinese}}.
Hence,we use crawler script to obtain the main-components and radicals information of those Chinese characters which is a total of 3500 from HTTPCN\footnote[1]{\url{http://tool.httpcn.com/zi/}}. 
In this step,we obtained 3433 main-components and 57 radicals.Furthermore,we treat the radicals as the main-components and finally we got 3491 main-components.
//we obtained 20,879 characters, 13,253 components and 218 radicals,of which 7,744 characters have more than one components, and 214 characters are equal to their radicals.
To create the word map,we need to obtain the explanatory meaning of the main-components of each characters.
Although the main-components are part of the characters, they are also simple Chinese characters with their own meaning.
Therefore, we have crawled the Chinese interpretation of 3491 main-components from HTTPCN and obtained the core latent meanings using manual annotation.
%Similarly, we obtained the core latent meaning of 57 radicals from XXX.
Although this process costs manpower and time, it could be done once and for all for each languag because it has the same knowledge base.
When we choose a Chinese word, the main-components it contains will be selected and further replaced by their latent meanings.
Taking into account the high efficiency of CBOW, our model is improved on the basis of CBOW.


Beselines:

For comparison,we chose two classic models including CBOW and GloVe and two component-level state-of-the-art character embedding models including CWE and GWE.
%Furthermore, we also selected two component-level state-of-the-art charactor embedding models including CWE and GWE for comparison.
In addition,we introduce a model Latent Meaning Model-Average(LMM-A),which uses the latent meanings of English morphemes to enhance the word embedddings. 
This model employ the morpheme embeddings to adjust the word embeddings of the target word in the training,
and assumes that all latent meanings has the same contribution to the target word.
We modified the source code of the LMM-A model to match our experiment.
In order to better verify the word analog performance of our model, we also selected two additional models named cw2vec and JWE respectively for word analogy experiments.
%//And take the average of all implied meanings as the weight of the contribution to the target word.

%//two word vector models CWE and GWE using Chinese component level.

Parameter Settings:
(-cbow 1 -size 200 -window 5 -negative 10 -hs 0 -sample 1e-4 -threads 25 -binary 0 -iter 15 -alpha 0.025)
For the sake of fairness, we used the same parameter settings for all models.
In order to speed up the training process, we have adopted negative sampling techniques for Cbow,CWE,GWE,LMM-A and our models.
We set the word vector dimension as 200, the window size as 5 and the subsampling parameter to be 1e-4. 
 We used 10-word negative sampling for optimization.



4.Evaluation Benchmarks
4.4.1 Word Similarity

This experiment is used to evaluate the semantic relevance of generated word embeddings in word pairs.
For Chinese word similarity, we employ two differnet manually-annotated datasets,wordsim-240 and wordsim-296 provided by (Chen et al., 2015).
These datasets are composed of word pairs and are manually labeled with the similarity scores for each word pair.
We utilize the cosine similarity to measure the similarity of each word pair, 
and the Spearman’s rank correlation coefficient (p) is employed to evaluate our calculation results and human scores.
More details of these datasets are shown in Table 1.


4.4.2 Syntactic Analogy

This task examines the quality of word embeddings by discovering the semantic inferential capability between pairs of words.
The core task of syntactic analogy is to answer the questions like "雅典(Athens) is to 希腊(Greece) as 东京(Tokyo) is to 日本(Japan)" where 日本(Japan) is the answer we hope to get.
This means that the model get the answer correctly if the similarity of "vector(希腊) - vector(雅典) + vector(东京)" and vector(日本) is the largest among all words.

（we use the Chinese word analogy dataset introduced by (Chen et al., 2015), the dataset is divided into adjectives, nouns and verbs.
which consists of 1,124 tuples of words and each tuple contains
4 words, coming from three different categories:“Capital” (677 tuples), “State” (175 tuples), and“Family” (272 tuples). Our training corpus covers
all the testing words.）出自jwe

It contains 3 analogy types: (1) capitals of countries (687 groups); (2) states/provinces of cities (175 groups); and (3) family words (240 groups). The learning corpus covers
more than 97% of all the testing words

The dataset contains 3 groups of
analogy problems: capitals of countries, (China)
states/provinces of cities, and family relations.
////A word analogy compares the relationship between two pairs of words



4.3Case Study
In addition to the above quantitative analysis, we also conducted a qualitative analysis of the impact of implied meanings.





5 The Impacts of Parameter Settings

The parameter settings can affect the performance of word embeddings.
We take a task to investigate the impact of corpus size for word embeddings.
In the analysis of corpus size, we set the same hyperparameters as before.
We select the Wordsim-353 as the evaluation standard of word similarity.
The entire corpus previously mentioned are divided into 1/5,2/5,3/5,4/5 and 5/5 respectively as our new corpus for the task.
As shown in Fig. 4,



6 Conclusion and Future Work
In this paper,we explored a new direction of using the latent meanings of Chinese internal components instead of themselves to enhance the Chinese word embeddings.
We propose three models named MCWE-Avg,MCWE-SA and MCWE-HA which make full use of subword information.
The attention model is used to dynamically adjust the weights of latent meanings for the main-components of Chinese characters in MCWE-SA and MCWE-HA.


//we introduce latent meanings of Chinese character's main-compoent into word embedding methods to enhance Chinese word embeddings.




Acknowledgments
The authors are grateful to the reviewers for constructive feedback.
We would like to thank the anonymous reviewers for their insightful comments and suggestions.


References


Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality.In Advances in neural information processing
systems, pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pages 746–751.


%CWE
\bibitem{ref1}Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,and Huanbo Luan. 2015. Joint learning of character and word embeddings. In Proceedings of IJCAI,pages 1236–1242.

%GWE
Tzu-Ray Su and Hung-Yi Lee. 2017. Learning chinese word representations from glyphs of characters. EMNLP.






