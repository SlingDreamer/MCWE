% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}

%
\usepackage[UTF8,nocap]{ctex}
\usepackage{booktabs} %更粗的表格线
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath}
%\usepackage[UTF8]{ctex}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%\bibliographystyle{plain}
%
\title{Enhancing Chinese Word Embeddings From Latent Meanings Of Chinese Characters\thanks{Supported by organization x.}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Xinyu Su\inst{1} \and
Second Author\inst{2,3} \and
Third Author\inst{3}}
%
\authorrunning{X. Su et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Department of Computer Science and Technology \and
University of Science and Technology of China, Hefei, 230027, China
\email{sa517303@mail.ustc.edu.cn}\\
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
%The abstract should briefly summarize the contents of the paper in
%150--250 words.
Word embeddings have a great impact on natural language processing.
In morpheme writing systems,most Chinese word embeddings takes a word as the basic unit, or directly use the internal structure of words.
However,these models still neglect the rich latent meanings in the internal structure of Chinese characters.
In this paper,we explore to employ the latent meanings of the main-components of the Chinese characters to train and enhance the Chinese word embeddings.
Based on this purpose,we propose three main-component enhanced word embedding models(MCWEs),named MCWE-Avg,MCWE-SA,MCWE-HA respectively,
which incorporate the latent meanings of the main-components during the training process.
Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our models.
Evaluations on both word similarity and word analogy tasks demonstrate the superior performance of our models.

\keywords{ Latent Meaning \and Component Level \and Word Embedding Enhancing.}
\end{abstract}
%
%
%
\begin{CJK}{UTF8}{gkai} 
\section{Introduction}
Word embedding, which is also termed distributed word representation, has been an important foundation in the field of Natural Language Processing (NLP)\cite{GWE}.
It encodes the semantic meaning of a word into a real-valued low-dimensional vector,which performs better in many tasks such as text classification, machine translation ..... (and so on) over traditional one-hot representations.
Among many word embedding models,Continuous Bag-of-Word (CBOW), Skip-gram (Mikolov et al.,2013a), Global Vectors (GloVe) (Penningtonet al., 2014) is popular because of their simplicity and efficiency.
(分布式假说)(morphemes/characters)
However, these models only focus on word level information, and do not pay attention to the fine-grained morphological information inside the words or the characters,such as componets of Chinese character or English
morphemes.
    Different from English words,Chinese characters are glyphs whose components may depict objects or represent abstract notions.Usually a character consists of more commonly two or more components 
which have meanings close to the character, using a variety of different principles\footnote[1]{\url{https://en.wikipedia.org/wiki/Written_Chinese}}.
That means Chinese words themselves are often composed of Chinese characters and subcharacter compoents,including rich semantic information.
    (引用论文)（过去式）

  Previous researchers have done some works by using the rich information inside Chinese for word embeddings enhancement with internal morphological semantics.
   (Li et al.  Component-Enhanced Chinese Character Embeddings) used the radicals to enhance the Chinese character embeddings.
(Chen et al. Joint Learning of Character and Word Embeddings) proposed the CWE model to improve the quality of Chinese word embeddings by exploit character level information.   
For a more fine-grained combination of Chinese character and radical,(Yin et al.Multi-Granularity Chinese Word Embedding)proposed methods to enhance Chinese character embeddings based on CWE model.
   (Jian et al.Improve Chinese Word Embeddings by Exploiting Internal Structure) used external language to calculate the similarity between Chinese words and characters to enhance semantic information
  based on the rich internal structure of Chinese words
(Monday4.22)
//In this work, we present a model to jointly learn the embeddings of Chinese words, characters,and subcharacter components.

(huang er al. Learning Chinese Word Representations From Glyphs Of Characters)proposed the GWE model,a pixel-based model that learns character features from font images to enhance the representation (of words with character glyphs).
(Yu et al.Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components) used Chinese characters and subcharacter components to improve Chinese word embeddings 
and proposed the JWE model to jointly learn Chinese word and character embeddings.
(Cao et al. cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information)proposed a cw2vec model, which exploits stroke-level information to improve the learning of Chinese word embeddings.


   However, the subcharacter compoents of a character contain a lot of unwanted noise information, so we integrate the word components into main-component.
Chinese characters contain elements and radicals，Just as Chinese contains Chinese characters.
The components of characters can be roughly divided into two types: main-component and radical.The main-component which consists of several components indicates the baisc meaning of a character while the radical indicates some attribute meanings of a character.
For example,"智 (intelligence)” is reduced into the components “矢(arrow)”, “口(mouth)” and “日(sun)”,and 矢(arrow) and 口(mouth) and are the components and 日(sun) is the radical of charactor 智.All these components may not be relevant to the semantics of the character.However, the main-component "知(wisdom)" consisting of several small components is closely related to the meaning of the charactor. Based on this, we have proposed .....

    In this paper, we have modified the CBOW model in the word2vec source code, introduced the information of the latent meaning of the Chinese main-component and radical, 
and proposed three efficient models called MCEW-Avg, MCWE-SA, and MCWE-HA.
   The learned Chinese word embeddings not only encode morphological properties into Chinese words, but also have a higher similarity to the synonym.
   Our model directly modifies embeddings of the target words, without generating and training extra embeddings for latent meanings.
In addition,we create a word map to describe the relationship between Chinese words and the latent meanings of their main-components and radicals.


Through our model, the rich implicit information in Chinese is fully utilized and the similarities between words and words is improved.
//Using our model, we can strengthen the similarity between words and words more effectively.
（贡献）1.构建汉语主成分map表；2.attention机制引用 3.有很强的拓展性（CWE）
Our contributions of this paper can be summarized as follows:

1.Rather than directly leverage the components of the word itself, we provide a method to use the latent meanings of the main-components and radicals to train the word embedding.
In order to verify the feasibility of our purpose,two models,named MCEW-SA and MCWE-HA,are proposed to incorporate the latent meanings.

2.We propose a method to assign the weights of latent meanings at input layer based on the attention scheme.
Through the attention mechanism, we focus on the latent meanings of higher contribution to the target word and assign them higher weights.

Subsequent paragraphs, however, are indented.
\end{CJK} 

\section{My model}
In this section, we introduce our Main-Component Word Embedding models(MCWEs),named Main-Component Word Embedding-Average(MCWE-Avg),MCWE-Soft Attention(MCWE-SA),MCWE-Hard Attention(MCWE-HA),
which is based on CBOW model(Mikolov et al., 2013a).
It should be noted that our models directly uses the internal meanings of the Chinese characters,rather than directly using the characters or componets of the word themselves.
MCWE-Avg assumes that all the latent meanings of the Chinese character's main-component to have the same weight.
However, most of the main-components are also a frequently-used Chinese word which may contain some ambiguous information.
To address this concern,we proposed the MCWE-SA which which based on the soft attention scheme.
The MCWE-SA assumes that the latent meanings of main-components have their own weights,and assign higher weights to the meanings closely related to the target character,
so that they have greater impact on the word embeddings.
What's more,MCWE-HA which based on the hard attention scheme only focuses on the latent meaning of the main-component with the highest similarity to the target word.
In what follows, we will introduce each of our MCWEs in detail.
At the end of this section, we are going to introduce the update rules of the models.
\subsection{MCWE-Avg:} We denote the Chinese character set as C and the Chinese word vocabulary as W. Each character ci-C is represented by vector ci, 
and each word wi-W is represented by vector wi.

We denote D as the training corpus, W =(w1;w2;.....;wN) as the vocabulary of words,C = (c1; c2;.... ; cM) as the vocabulary of characters. 

The item for wi in the word map is wi -- Mi. Mi is a set of latent meanings of wi's main-components, 
(Particularly,we treat the radical as a main-component and add the latent meaning of the radical to the main-component latent meaning.)
Formally, a context word wj is represented as
公式见本子

\subsection{MCWE-SA:}
Through observation, we found that most Chinese words have more than one latent meaning with their main-component,
but some latent meanings have low correlation with target words(the corresponding word).For example,main-component "知(knowledge)" means "" 、"" and "".
As Fig.3 shows,for the item "智慧(intelligence)" ——{[],[],[],[],[],[],[],[]} in the word map,each latent meaning has a bias on the word "智慧(intelligence)".
Therefore, we assign different weights to each latent meaning based on the idea of soft attention model.
We measure the weights of latent meanings by calculating the cosine similarity between the corresponding latent meanings and the target word where cosine similarity is usually used to measure the similarity between word embeddings.
Furthermore,we remove the negatively correlated latent meaning.
Hence, at the input layer, the modified embedding of xi can be expressed as
\begin{equation}
\begin{aligned}
\hat v_{x_{i}} &= \frac{1}{2}\{v_{x_{i}}+\frac{1}{N_i}\sum_{k=1}^{N_i}sim(x_i,m_k)\cdot m_k  \},\\
sim(&x_i,m_k)=cos(v_{x_i},v_{m_k}), \quad  s.t. \    cos(v_{x_i},v_{m_k}) > 0
%a\overset{~}{x}b
\end{aligned}
\end{equation}
Let Sim(·) denotes the function to calculate the similarity between meanings of Chinese words and characters, we use cosine distance as the distance metric.
The i-th latent meaning of the main-component of Chinese character c are ci
\subsection{MCWE-HA:}
//In order to further reduce the problem that the main component implies the opposite of the similarity of Chinese characters, we propose MCWE-HA.
In order to further reduce the impact of irrelevant latent meanings to the word, we propose MCWE-HA which is based on the idea of hard attention model.
We only choose the latent meaning which have the greatest similarity to the Chinese word.
According to experimental experience, we only retain the latent meaning of similarity with token xi greater than 0.5.
And MCWE-HA is mathematically defined as
\begin{equation}
\begin{aligned}
\hat v_{x_{i}} &= \frac{1}{2}\(v_{x_{i}}+v_{m_{max}} \),\\
m_{max}&=\mathop{argmax}\limits_{M} cos(v_{x_i},v_{t}),  t\in{M_i}  \\
 s.t. \qquad      cos(&v_{x_i},v_{m_k}) > 0.95
\end{aligned}
\end{equation}

where we only keep values with cos（Vxi,Vm） greater than 0.5 for comparison. 
\section{Experiments}
In this section,we test the performance of our model in generating high-quality word embeddings on word similarity evaluation and word analogy tasks.
\subsection{Experimental Settings}
\subsubsection{Traing Corpus:}
We utilize a medium-sized Chinese corpus to train all word embedding models.We adopt the Chinese Wikipedia Dump as our training corpus whose size is about 7GB.
We use a script named WikiExtractor to convert data from XML into text format.What's more,We use THULAC 3 (Sun et al.,2016b) for Chinese word segmentation and normalize all characters as simplified Chinese.
In pre-processing,we filter all digits,punctuation masks and non Chinese characters.
To get better quality of the word embeddings, we removed the stop words (that are common\_ which from ) in the corpus.
Finally, we obtained a training corpus of approximately 2g in size, containing 2123123 words, and 213 unique words.
\subsubsection{Word Map:}
The Modern Chinese Word List is divided into two parts: 2500 common characters and 1000 secondary common characters,and the coverage of those words in most corpora reached 99.48% which means mastering the common and secondary words has reached the basic requirements for using Chinese.
Hence,we use crawler script to obtain the main-components and radicals information of those Chinese characters which is a total of 3500 from HTTPCN. We obtained 3433 main-components and 57 radicals.
//we obtained 20,879 characters, 13,253 components and 218 radicals,of which 7,744 characters have more than one components, and 214 characters are equal to their radicals.
To create the word map,we need to obtain the explanatory meaning of the main-components of each characters.
Although the main-components are part of the characters, they are themselves characters.
Therefore, we have crawled the Chinese interpretation of 3433 main-component characters from HTTPCN and obtained the core latent meanings using manual annotation.
Similarly, we obtained the core latent meaning of 57 radicals from XXX.
Although this process costs manpower and time, it can be done once and for all for each languag because it has the same knowledge base.
When we choose a Chinese charactor,its main-component and radical will be selected and further replaced by their latent meanings.

\paragraph{Sample Heading (Fourth Level)}
The contribution should contain no more than four levels of
headings. Table~\ref{tab1} gives a summary of all heading levels.

%https://blog.csdn.net/JueChenYi/article/details/77116011
\begin{table}[!htbp]
\caption{Spearman’s Coefficients of word similarity on three data sets.}\label{tab1}
\begin{tabular}{ccc}
\toprule
 \textbf{Model} &  \textbf{Wordsim-240} &\textbf{Wordsim-296}  \\
\hline
Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
\bottomrule %添加表格底部粗线
\end{tabular}
\end{table}


\noindent Displayed equations are centered and set on a separate
line.
\begin{equation}
x + y = z
\end{equation}
Please try to avoid rasterized images for line-art diagrams and
schemas. Whenever possible, use vector graphics instead (see
Fig.~\ref{fig1}).

\begin{figure}
\includegraphics[width=\textwidth]{fig1.eps}
\caption{A figure caption is always placed below the illustration.
Please note that short captions are centered, while long ones are
justified by the macro package automatically.} \label{fig1}
\end{figure}

\begin{theorem}
This is a sample theorem. The run-in heading is set in bold, while
the following text appears in italics. Definitions, lemmas,
propositions, and corollaries are styled the same way.
\end{theorem}
%
% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%
\begin{proof}
Proofs, examples, and remarks have the initial word in italics,
while the following text appears in normal font.
\end{proof}
For citations of references, we prefer the use of square brackets
and consecutive numbers. Citations using labels or the author/year
convention are also acceptable. The following bibliography provides
a sample reference list with entries for journal
articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
and a homepage~\cite{ref_url1}. Multiple citations are grouped
\cite{ref_article1,ref_lncs1,ref_book1},
\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\begin{thebibliography}{8}
\bibitem{ref_article1}
Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

\bibitem{ref_lncs1}
Author, F., Author, S.: Title of a proceedings paper. In: Editor,
F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
Springer, Heidelberg (2016). \doi{10.10007/1234567890}

\bibitem{ref_book1}
Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
Location (1999)

\bibitem{ref_proc1}
Author, A.-B.: Contribution title. In: 9th International Proceedings
on Proceedings, pp. 1--2. Publisher, Location (2010)

\bibitem{ref_url1}
LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
Oct 2017
\end{thebibliography}
\end{document}
